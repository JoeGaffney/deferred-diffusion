services:
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    tty: true
    stdin_open: true
    ports:
      - "5000:5000"
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - DDIFFUSION_API_KEYS=${DDIFFUSION_API_KEYS}
      - OPENAI_API_KEY=${OPENAI_API_KEY}

  # Redis service as message broker
  redis:
    image: redis:latest
    ports:
      - "6379:6379"

  # Celery worker service
  gpu-workers:
    command: celery -A worker worker --loglevel=info --pool=threads --concurrency=1 -Q gpu
    build:
      context: .
      dockerfile: Dockerfile.workers
    image: deferred-diffusion-workers
    tty: true
    stdin_open: true
    volumes:
      - hf_cache:/WORKSPACE
      - Y:/TEST_OUTPUTS:/app/tmp
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - CELERYD_PREFETCH_MULTIPLIER=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - RUNWAYML_API_SECRET=${RUNWAYML_API_SECRET}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/WORKSPACE
      - TORCH_HOME=/WORKSPACE
      - MAX_MODEL_CACHE=1 # Maximum number of models to cache in memory
      - LOW_VRAM=0 # Set to true to enable low VRAM mode
      - HF_ENABLE_PARALLEL_LOADING=yes
      # - HF_HUB_DISABLE_XET=1 # Disable X-ET headers for Hugging Face Hub
    depends_on:
      - redis
      - api
    deploy:
      replicas: 1 # Number of container
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  cpu-workers:
    command: celery -A worker worker --concurrency=4 --loglevel=info -Q cpu
    build:
      context: .
      dockerfile: Dockerfile.workers
    image: deferred-diffusion-workers
    tty: true
    stdin_open: true
    volumes:
      - hf_cache:/WORKSPACE
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - CELERYD_PREFETCH_MULTIPLIER=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - RUNWAYML_API_SECRET=${RUNWAYML_API_SECRET}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/WORKSPACE
      - TORCH_HOME=/WORKSPACE
    depends_on:
      - redis
      - api
    deploy:
      replicas: 1 # Number of container

  flower:
    image: mher/flower:latest
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    ports:
      - "5555:5555"
    depends_on:
      - gpu-workers
      - cpu-workers
      - redis

  # comfy ui for creating and managing workflows
  comfy:
    build:
      context: .
      dockerfile: Dockerfile.comfy

    volumes:
      - Y:/COMFY/models:/app/ComfyUI/models # For user access to models
      - Y:/COMFY/custom_nodes:/app/ComfyUI/custom_nodes
      - Y:/COMFY/user:/app/ComfyUI/user # For user settings/configs
      - Y:/COMFY/output:/app/ComfyUI/output # For output files
    ports:
      - "8188:8188" # expose ComfyUI web UI port
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Model loader service to sync models from a shared directory
  model-loader:
    image: alpine:latest
    volumes:
      - Y:/HF_HOME:/source:ro
      - hf_cache:/destination
    environment:
      - HF_HOME=/WORKSPACE
    command: >
      sh -c "
        echo 'Setting up...' &&
        apk add --no-cache rsync &&
        echo 'Starting models sync...' &&
        rsync -av /source/ /destination/ &&
        echo 'Sync complete!'
      "

# Volume definitions go at the root level, not nested inside the service
volumes:
  hf_cache:
    name: deferred-diffusion_hf_cache
