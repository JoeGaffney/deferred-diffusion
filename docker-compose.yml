services:
  # API service (stateless)
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    tty: true
    stdin_open: true
    ports:
      - "5000:5000"
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - DDIFFUSION_API_KEYS=${DDIFFUSION_API_KEYS}

  # Model Context Protocol service - for allowing LLM tools
  mcp:
    build:
      context: .
      dockerfile: Dockerfile.api
    command: fastmcp run mcp_server.py:mcp --transport http --host 0.0.0.0 --port 5001
    tty: true
    stdin_open: true
    ports:
      - "5001:5001"
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - DDIFFUSION_API_KEYS=${DDIFFUSION_API_KEYS}
      - FASTMCP_EXPERIMENTAL_ENABLE_NEW_OPENAPI_PARSER=true

  # Redis service as message broker
  redis:
    image: redis:latest
    ports:
      - "6379:6379"

  # Celery worker services
  gpu-workers:
    command: celery -A worker worker --loglevel=info --pool=threads --concurrency=1 -Q gpu
    build:
      context: .
      dockerfile: Dockerfile.workers
    image: deferred-diffusion-workers
    tty: true
    stdin_open: true
    volumes:
      - hf_cache:/WORKSPACE
      - Y:/TEST_OUTPUTS:/app/tmp
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - CELERYD_PREFETCH_MULTIPLIER=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - RUNWAYML_API_SECRET=${RUNWAYML_API_SECRET}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/WORKSPACE
      - TORCH_HOME=/WORKSPACE
      - MAX_MODEL_CACHE=1 # Maximum number of models to cache in memory
      - HF_ENABLE_PARALLEL_LOADING=yes
      - VIDEO_CPU_OFFLOAD=0 # Offload video models to CPU when not in use
      # - HF_HUB_DISABLE_XET=1 # Disable XET when downloading from HF Hub
    depends_on:
      - redis
      - api
    deploy:
      replicas: 1 # Number of container
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  cpu-workers:
    command: celery -A worker worker --concurrency=4 --loglevel=info -Q cpu
    build:
      context: .
      dockerfile: Dockerfile.workers
    image: deferred-diffusion-workers
    tty: true
    stdin_open: true
    volumes:
      - hf_cache:/WORKSPACE
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - CELERYD_PREFETCH_MULTIPLIER=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - RUNWAYML_API_SECRET=${RUNWAYML_API_SECRET}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/WORKSPACE
      - TORCH_HOME=/WORKSPACE
    depends_on:
      - redis
      - api
    deploy:
      replicas: 1 # Number of container

  # Task monitoring service
  flower:
    image: mher/flower:latest
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    ports:
      - "5555:5555"
    depends_on:
      - gpu-workers
      - cpu-workers
      - redis

  # Agentic orchestration and UI service
  agentic:
    build:
      context: .
      dockerfile: Dockerfile.agentic
    ports:
      - "7000:7000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DDIFFUSION_API_KEY=${DDIFFUSION_API_KEY}
      - DDIFFUSION_MCP_ADDRESS=http://mcp:5001/mcp
    depends_on:
      - mcp

  # Model loader service to sync models from a shared directory
  model-loader:
    image: alpine:latest
    volumes:
      - Y:/HF_HOME:/source:ro
      - hf_cache:/destination
    environment:
      - HF_HOME=/WORKSPACE
    command: >
      sh -c "
        echo 'Setting up...' &&
        apk add --no-cache rsync &&
        echo 'Starting models sync...' &&
        rsync -av /source/ /destination/ &&
        echo 'Sync complete!'
      "

# Volume definitions go at the root level, not nested inside the service
volumes:
  hf_cache:
    name: deferred-diffusion_hf_cache
