services:
  # API service (stateless)
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    tty: true
    stdin_open: true
    ports:
      - "5000:5000"
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - DDIFFUSION_API_KEYS=${DDIFFUSION_API_KEYS}

  # Model Context Protocol service - for allowing LLM tools
  mcp:
    build:
      context: .
      dockerfile: Dockerfile.api
    command: fastmcp run mcp_server.py:mcp --transport http --host 0.0.0.0 --port 5001
    tty: true
    stdin_open: true
    ports:
      - "5001:5001"
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - DDIFFUSION_API_KEYS=${DDIFFUSION_API_KEYS}
      - FASTMCP_EXPERIMENTAL_ENABLE_NEW_OPENAPI_PARSER=true

  # Redis service as message broker
  redis:
    image: redis:latest
    ports:
      - "6379:6379"

  # Celery worker services
  gpu-workers:
    command: celery -A worker worker --loglevel=info --pool=threads --concurrency=1 -Q gpu,comfy
    build:
      context: .
      dockerfile: Dockerfile.workers
    image: deferred-diffusion-workers
    networks:
      - default
      - comfy_isolated # Can reach comfy service
    tty: true
    stdin_open: true
    volumes:
      - hf_cache:/WORKSPACE
      - ${OUTPUTS_PATH:-Y:/OUTPUTS}:/tmp/deferred-diffusion
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - CELERYD_PREFETCH_MULTIPLIER=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/WORKSPACE
      - TORCH_HOME=/WORKSPACE
      - HF_ENABLE_PARALLEL_LOADING=yes
      - COMFY_API_URL=http://comfy:8188
    depends_on:
      - redis
      - api
    deploy:
      replicas: 1 # Number of container
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  cpu-workers:
    command: celery -A worker worker --concurrency=4 --loglevel=info -Q cpu
    build:
      context: .
      dockerfile: Dockerfile.workers
    image: deferred-diffusion-workers
    networks:
      - default
      - comfy_isolated # Can reach comfy service
    tty: true
    stdin_open: true
    volumes:
      - hf_cache:/WORKSPACE
      - ${OUTPUTS_PATH:-Y:/OUTPUTS}:/tmp/deferred-diffusion
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - CELERYD_PREFETCH_MULTIPLIER=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/WORKSPACE
      - TORCH_HOME=/WORKSPACE
    depends_on:
      - redis
      - api
    deploy:
      replicas: 1 # Number of container

  # ComfyUI service (sidecar for GPU workers)
  comfy:
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
    build:
      context: .
      dockerfile: Dockerfile.comfy
    volumes:
      - comfy_models:/app/ComfyUI/models:rw # For user access to models - make :ro in production if needed
      - comfy_custom_nodes:/app/ComfyUI/custom_nodes:rw # For user access to custom nodes - make :ro in production if needed
      - comfy_user:/app/ComfyUI/user:rw # For user access to user data - make :ro in production if needed
    ports:
      - "127.0.0.1:8188:8188" # Localhost only
    networks:
      - comfy_isolated # Separate network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Task monitoring service
  flower:
    image: mher/flower:latest
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    ports:
      - "5555:5555"
    depends_on:
      - gpu-workers
      - cpu-workers
      - redis

  # Agentic orchestration and UI service
  agentic:
    build:
      context: .
      dockerfile: Dockerfile.agentic
    ports:
      - "7000:7000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DDIFFUSION_API_KEY=${DDIFFUSION_API_KEY}
      - DDIFFUSION_MCP_ADDRESS=http://mcp:5001/mcp
    depends_on:
      - mcp

  # ComfyUI sync service - change paths as needed
  comfy-sync:
    image: alpine:latest
    volumes:
      - ${COMFY_MODELS_PATH:-Y:/COMFY/models}:/source_models:ro
      - ${COMFY_NODES_PATH:-Y:/COMFY/custom_nodes}:/source_nodes:ro
      - comfy_models:/dest_models
      - comfy_custom_nodes:/dest_nodes
    command: >
      sh -c "
        echo 'Setting up...' &&
        apk add --no-cache rsync &&
        echo 'Syncing custom nodes...' &&
        rsync -av /source_nodes/ /dest_nodes/ &&
        echo 'Syncing models...' &&
        rsync -av /source_models/ /dest_models/ &&
        echo 'ComfyUI sync complete!'
      "

networks:
  default:
  comfy_isolated:

# Volume definitions go at the root level, not nested inside the service
volumes:
  hf_cache:
    name: deferred-diffusion_hf_cache
  comfy_models:
    name: deferred-diffusion_comfy_models
  comfy_custom_nodes:
    name: deferred-diffusion_comfy_custom_nodes
  comfy_user:
    name: deferred-diffusion_comfy_user
