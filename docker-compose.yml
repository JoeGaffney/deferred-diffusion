services:
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    tty: true
    stdin_open: true
    ports:
      - "5000:5000"
    # volumes:
    #   - hf_cache:/WORKSPACE # Using Docker named volume
    environment:
      - DEF_DIF_API_KEYS=${DEF_DIF_API_KEYS}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PYTHONUNBUFFERED=1
      # - HF_HOME=/WORKSPACE
      # - TORCH_HOME=/WORKSPACE
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1

  # Redis service as message broker
  redis:
    image: redis:latest
    ports:
      - "6379:6379"

  # Celery worker service - for now comfy runs in the monolith worker
  workers:
    build:
      context: .
      dockerfile: Dockerfile.workers
    tty: true
    stdin_open: true
    volumes:
      - hf_cache:/WORKSPACE # Using Docker named volume
      # - Y:/COMFY/models:/app/ComfyUI/models # this is slow on Windows
      - comfy_models:/app/ComfyUI/models # Using Docker named volume is much faster than bind mount but requires a copy
      - Y:/COMFY/custom_nodes_worker:/app/ComfyUI/custom_nodes
      - Y:/COMFY/output:/app/ComfyUI/output # For output files
    environment:
      - PYTHONUNBUFFERED=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - RUNWAYML_API_SECRET=${RUNWAYML_API_SECRET}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/WORKSPACE
      - TORCH_HOME=/WORKSPACE
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - CELERYD_PREFETCH_MULTIPLIER=1
    depends_on:
      - redis
      - api
    deploy:
      replicas: 1 # Number of container
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  flower:
    image: mher/flower:latest
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    ports:
      - "5555:5555"
    depends_on:
      - workers
      - redis

  # comfy ui for creating and managing workflows
  comfy:
    build:
      context: .
      dockerfile: Dockerfile.comfy

    volumes:
      - Y:/COMFY/models:/app/ComfyUI/models # For user access to models
      - Y:/COMFY/custom_nodes:/app/ComfyUI/custom_nodes
      - Y:/COMFY/user:/app/ComfyUI/user # For user settings/configs
      - Y:/COMFY/output:/app/ComfyUI/output # For output files
    ports:
      - "8188:8188" # expose ComfyUI web UI port
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Model loader service to sync models from a shared directory
  model-loader:
    image: alpine:latest
    volumes:
      - Y:/COMFY/models:/source:ro
      - comfy_models:/destination
    command: >
      sh -c "
        echo 'Setting up...' &&
        apk add --no-cache rsync &&
        echo 'Starting models sync...' &&
        rsync -av --delete /source/ /destination/ &&
        echo 'Sync complete!'
      "

# Volume definitions go at the root level, not nested inside the service
volumes:
  hf_cache: {}
  comfy_models: {}
