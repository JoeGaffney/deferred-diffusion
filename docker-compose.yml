services:
  # API service (stateless)
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    tty: true
    stdin_open: true
    ports:
      - "5000:5000"
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - DDIFFUSION_API_KEYS=${DDIFFUSION_API_KEYS}

  # Model Context Protocol service - for allowing LLM tools
  mcp:
    build:
      context: .
      dockerfile: Dockerfile.api
    command: fastmcp run mcp_server.py:mcp --transport http --host 0.0.0.0 --port 5001
    tty: true
    stdin_open: true
    ports:
      - "5001:5001"
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - DDIFFUSION_API_KEYS=${DDIFFUSION_API_KEYS}
      - FASTMCP_EXPERIMENTAL_ENABLE_NEW_OPENAPI_PARSER=true

  # Redis service as message broker
  redis:
    image: redis:latest
    ports:
      - "6379:6379"

  # Celery worker services
  gpu-workers:
    command: celery -A worker worker --loglevel=info --pool=threads --concurrency=1 -Q gpu
    build:
      context: .
      dockerfile: Dockerfile.workers
    image: deferred-diffusion-workers
    tty: true
    stdin_open: true
    volumes:
      - hf_cache:/WORKSPACE
      - Y:/OUTPUTS:/tmp/deferred-diffusion
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - CELERYD_PREFETCH_MULTIPLIER=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/WORKSPACE
      - TORCH_HOME=/WORKSPACE
      - HF_ENABLE_PARALLEL_LOADING=yes
      - COMFY_API_URL=http://comfy:8188
    depends_on:
      - redis
      - api
    deploy:
      replicas: 1 # Number of container
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  cpu-workers:
    command: celery -A worker worker --concurrency=4 --loglevel=info -Q cpu
    build:
      context: .
      dockerfile: Dockerfile.workers
    image: deferred-diffusion-workers
    tty: true
    stdin_open: true
    volumes:
      - hf_cache:/WORKSPACE
      - Y:/OUTPUTS:/tmp/deferred-diffusion
    environment:
      - PYTHONUNBUFFERED=1
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
      - CELERYD_PREFETCH_MULTIPLIER=1
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REPLICATE_API_TOKEN=${REPLICATE_API_TOKEN}
      - HF_TOKEN=${HF_TOKEN}
      - HF_HOME=/WORKSPACE
      - TORCH_HOME=/WORKSPACE
    depends_on:
      - redis
      - api
    deploy:
      replicas: 1 # Number of container

  comfy:
    build:
      context: .
      dockerfile: Dockerfile.comfy
    volumes:
      - comfy_models:/app/ComfyUI/models:ro # For user access to models
      # not make read-only later - possibly split into two containers
      - Y:/COMFY/custom_nodes:/app/ComfyUI/custom_nodes # For custom nodes
      - Y:/COMFY/user:/app/ComfyUI/user # For user settings/configs
      - Y:/COMFY/output:/app/ComfyUI/output # For output files
    ports:
      - "8188:8188" # expose ComfyUI web UI port
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Task monitoring service
  flower:
    image: mher/flower:latest
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/1
    ports:
      - "5555:5555"
    depends_on:
      - gpu-workers
      - cpu-workers
      - redis

  # Agentic orchestration and UI service
  agentic:
    build:
      context: .
      dockerfile: Dockerfile.agentic
    ports:
      - "7000:7000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DDIFFUSION_API_KEY=${DDIFFUSION_API_KEY}
      - DDIFFUSION_MCP_ADDRESS=http://mcp:5001/mcp
    depends_on:
      - mcp

  # Model loader service to sync models from a shared directory
  comfy-model-loader:
    image: alpine:latest
    volumes:
      - Y:/COMFY/models:/source:ro
      - comfy_models:/destination
    command: >
      sh -c "
        echo 'Setting up...' &&
        apk add --no-cache rsync &&
        echo 'Starting models sync...' &&
        rsync -av /source/ /destination/ &&
        echo 'Sync complete!'
      "

# Volume definitions go at the root level, not nested inside the service
volumes:
  hf_cache:
    name: deferred-diffusion_hf_cache
  comfy_models:
    name: deferred-diffusion_comfy_models
